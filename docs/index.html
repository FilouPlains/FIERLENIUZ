<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <title>FIERLENIUS</title>
    <link rel="stylesheet" href="style.css" />
    <link rel="icon" type="image/x-icon" href="jupyter_logo_icon.svg" />
    
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body id="main">
    <header style="text-align: center;">
        <h1>‚åõÔ∏è FIERLENIUS ü¶ô</h1>

        <p><strong><em>‚úç Authors:</em> ROUAUD Lucas</strong></p>
        <p><strong><em>Master 2 Bio-informatics at Univerit√© de Paris</em></strong></p>

        <a href="https://www.python.org/downloads/release/python-397/"><img src="https://img.shields.io/badge/python-%E2%89%A5_3.10.8-blue.svg" alt="Python 3.10.8"></a>
        <a href="https://docs.conda.io/en/latest/miniconda.html"><img src="https://img.shields.io/badge/miniconda-%E2%89%A5_22.11.1-green.svg" alt="Conda 22.11.1"></a>
        <a href="https://github.com/FilouPlains/FIERLENIUS"><img src="https://img.shields.io/github/last-commit/FilouPlains/FIERLENIUS.svg" alt="GitHub last commit"></a>
    </header>

    <main>
        <section>
            <h1>Natural language processing (NLP)</h1>
            
            <div>
                <h2>Overall functioning of the Word2Vec package</h2>

                <p>Let's considerate a corpus. In it, there are sentences like so:</p>
                <div class="pre">
I am a sentence.
I can be analyzed any time!
                </div>

                <p>
                    In NLP, we would like to be able to guess words context. Why?
                    Because like so, we would be able to bring  closer together
                    words with the same meaning, while words with different ones
                    while be discarded. In order to represent this kind of context,
                    we can use vectors:
                    <ul>
                        <li>
                            With same context, the <code>dot product</code> of two
                            vector while give a small value (in case of measuring 
                            a distance).
                        </li>
                        <li>
                            With same context, the <code>dot product</code> of two
                            vector while give a high value (in case of measuring 
                            a distance).
                        </li>
                    </ul>
                </p>

                <p>
                    To make this vectors, we can use neural network with <strong>
                    Word2Vec</strong>. Done by <em>Mikolov et al. in 2013 (arXiv)</em>,
                    this allow to give in input a corpus on which the neural network
                    will trained, to then output our vectors, known as embeddings.
                    First, to define a context, we can see that this is the words,
                    in in limited windows, around a central word. Like so:
                </p>
                <p class="img">
                    <img src="svg/context_scheme.svg" />
                    <br />
                    The red word is the central word. Its context is defined by 
                    is surrounding, the black word.
                </p>
                <p>
                    So here, we base ourself on the surrounding of &ldquo;a&rdquo;
                    to guess its context and meaning. So next, for our corpus, we
                    are going to give it to the neural network, which is a CBOW
                    architecture:
                </p>
                <p class="img">
                    <img src="svg/CBOW.svg" style="width:25cm;" />
                    <br />
                    Here, each context word are used to output a new vector: the
                    central word. For this, we update our weight to match the hot
                    encoding embedding. With a loss function (average negative log
                    likelihood), we minimize it to increase the prediction accuracy.
                    In other words, we combine all context vector to output the
                    central vector words with weight on those first ones.
                </p>

                <h2>Application on pyHCA</h2>

                <p>
                    <code>pyHCA</code> is a software that cut a sequence in foldable
                    part. These cutting are based on the density in hydrophobic
                    cluster. With those hydrophobic cluster, we can obtain Peitsch
                    code. Based on this, we can make a corpus composed of Peitsch
                    code. The dataset used will be the SCOPe one. We only keep the
                    globular soluble domain. Like so, we can considerate two types
                    of sentences:

                    <ol>
                        <li>One sentence = one domain.</li>
                        <li>One sentence = foldable domaine.</li>
                    </ol>
                </p>
                <p class="img">
                    <img src="svg/domain_to_peitsch.svg" style="width:12cm;" />
                    <br />
                    <strong>Top line:</strong> Domain with, in bold, the foldable
                    segment. <strong>Middle line:</strong> The hydrophobic cluster.
                    <strong>Bottom line:</strong> Code Peitsch.
                </p>
                <div class="pre">
|‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî|
|                   DOMAIN                  |
|‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî|
| FOLDABLE SEGMENT 1 |  FOLDABLE SEGMENT 2  |
|‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî|
|   code 1 code 2    | code 3 code 4 code 5 |
|‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî|

>Cutting by [DOMAIN]:
    - SENTENCE 1 = code 1 code 2 code 3 code 4 code 5
    
>Cutting by [FOLDABLE SEGMENT]:
    - SENTENCE 1 = code 1 code 2
    - SENTENCE 2 = code 3 code 4 code 5
                </div>
                <p>
                    The TOP representation indicates how, based on domain or foldable
                    segment, we can obtain to set of sentences. Then, we give this
                    corpus to Word2Vec to obtain our embedding. With these embedding,
                    we can perform 2D projections or cosine distance on matrix.
                </p>
            </div>
        </section>

        <section>
            <a href="embedding/projection.html"><h1>Projection</h1></a>

            <div>
                <h2>2D projection methods principle</h2>
                <h3>PCA</h3>
                
                <p>
                    The Principal Component Analysis is a method to reduce dimensionality
                    of large dataset. In order to do so, we search components (vector-like)
                    that maximizes the variance. If we imagine a ellipsoid (3D ellipse),
                    this will mean seeking for the line going through the larger
                    ellipsoid's axe.
                </p>

                <h3>MDS</h3>

                <p>
                    The Multidimensional scaling create an embedding space where
                    the data dimension are reduced to 2. The MDS try to keep distance
                    between two point. To do so, the MDS methods try to minimize:

                    \[ J = \displaystyle\sum_{i,~j} \left( D_{ij} - d_{ij} \right)^2 \]

                    With <code>D</code> the original distance between two points in the original
                    space projections and <code>d</code> the new distance between
                    the two points in the new space projections. The embedding try
                    to to this. MDS is an iterative method.
                </p>
                
                <h3>t-SNE</h3>

                <p>
                    For th t-SNE principle, there is some common points with MDS.
                    Because, in a global view, they both try to minimize a criteria
                    and output an embedding.
                </p>
                <p>
                    The t-distributed Stochastic Neighbour Embedding start with
                    a PCA before trying to find, iteratively the minimum of its
                    criteria. Here, t-SNE try to keep the distance between neighbors.
                    This allow to emphasize the data clustering. The criteria that
                    is minimize is:

                    \[ KL (P || Q) = \displaystyle\sum_{i \neq j} p_{ij} \times log \left( \dfrac{p_{ij}}{q_{ij}} \right) \]

                    Which is a metric to evaluate a distance to see how distribution
                    (probability) are different between P and Q. Those two are from
                    the computation of distribution of hight and low dimensional
                    objects. In other words, we try to give probabilities such as
                    close objects have high probability value and vice verse.
                </p>

                <h3>UMAP</h3>
                <p>
                    Uniform Manifold Approximation and Projection is a method very
                    similar to t-SNE. Its changed the way of gathering points together
                    based on graph methods.
                </p>
                
                <h3>Sammon mapping</h3>
                <p>
                    Sammon mapping is like MDS, but its try to preserve data structures,
                    so the inter-point distance, much than MDS. The different between
                    those methods are mainly due to the minimize function call Sammon's
                    stress or error :

                    \[ E = \dfrac{1}{\displaystyle\sum_{i < j} D_{ij}} \times \left( \displaystyle\sum_{i < j} \dfrac{\left( D_{ij} - d_{ij} \right)^2}{D_{ij}} \right)\]

                    With <code>D</code> the original distance between two points in
                    the original space projections and <code>d</code> the new distance
                    between the two points in the new space projections.
                </p>

                <h2>Projection interpretation</h2>


            </div>
        </section>

        <section>
            <a href="embedding/matrix.html"><h1>Matrix</h1></a>

            <div>
                <p>‚ö†Ô∏è <strong>HEAVY LOADING PAGE, DUE TO THE LOADING HEATMAP</strong></p>
                <h2>Cosine similarity</h2>

                <p>
                    With the obtain embedding, it is possible to compute a cosine
                    similarity like so :
                    
                    \[ cos(\theta) = \dfrac{\vec{A} \times \vec{B}}{\lVert \vec{A} \rVert \times \lVert \vec{B} \rVert} \]
                    
                    To explain simply, the top fraction compute the aforementioned
                    similarity with a dot product. The more the value are similar,
                    the more the angle will be null, the more it will tend to 1.
                    The denominator is here to get rid of the vector size. After,
                    to get the distance, we have to do:
                    
                    \[ distance = 1 - |cos(\theta)| \]
                    
                    So with this, when two Peitsch code will have a same context,
                    the distance will tend to 0.
                </p>

                <h2>Matrix interpretation</h2>

                <p>
                    To <strong>interprate the matrix</strong>, we have to understand
                    what they are. It's about a distance obtain with a cosine similarity.
                    This last one was also obtain with the <em>words embeddings</em>,
                    describe in the first part. What the embeddings give is a context.
                    When two words have the same one, their similarities are high,
                    which mean that their distance is low. So two same words have
                    a similraity of 1 and a distance of 0. This mean that, when
                    two words have the same extact context, they are likely the
                    same, in other words synonyms. Here, because we have hydrophobics
                    clusters, that means that those are interchangeable or substituable.
                    So more a distance is high, more two hydrophobics clusters are
                    different and not substituable and vice verse.
                </p>
                <p>
                    <a href="embedding/matrix.html#Without---segment"><strong>Here,
                    you can see an example of output matrix.</strong></a>
                </p>
            </div>
        </section>

        <section>
            <a href="embedding/comparing_distribution.html"><h1>Distribution</h1></a>

            <div>
                <h2>Testing two distributions in HCDB vs SCOPe</h2>
                <h3>Testing data normality</h3>

                <p>
                    The distribution are shown in 
                    <strong><a href="embedding/comparing_distribution.html#Plotting-commons-distributions">[this part]</a></strong>.
                </p>
                <p>
                    In both case, the obtained p-value is strictly inferior to 0.001
                    <strong>(so highly significant ***)</strong>. This means that
                    the null hypothesis H0 is rejected, meaning that the data do
                    not follow a normal distribution. This means that the test to
                    effectuate next to test the distribution is a non-parametric
                    one.
                </p>
                
                <h3>Testing the distribution</h3>

                <p>
                    The Kolmogorov‚ÄìSmirnov test check if two distributions are the
                    same. In this case, the obtained p-value is strictly inferior
                    to 0.001 <strong>(so highly significant ***)</strong>. This
                    means that the null hypothesis H0 is rejected, meaning that
                    the data do not follow the same distribution. Those results
                    can be explained in two different ways:

                    <ol>
                        <li>
                            Data from rarest sample (in other words, from high Peitsch
                            code) are very different and instable. This influence
                            a lot the data.
                        </li>
                        <li>
                            The count of small Peitsch codes is more invariant.
                            Also, we have more data in a case than the other,
                            influencing also the distribution.
                        </li>
                    </ol>
                </p>
            </div>
        </section>
    </main>
</body>

</html>
